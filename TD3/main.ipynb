{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Work 3 : ...\n",
    "\n",
    "**Authors:** CHRETIEN Jérémy, DAVIDSON Colin, LAFAGE Adrien, REMBUSCH Gabrielle and WILBRINK Aurore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/eisti/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/eisti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eisti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eisti/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as stpw\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string \n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"train_dataset.csv\")\n",
    "test_set = pd.read_csv(\"test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing n°1\n",
    "\n",
    "We used lemmatization to reduce words to their base form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_v1(content) : \n",
    "\n",
    "    # remove upper letters\n",
    "    content = content.lower()\n",
    "    \n",
    "    # remove punctuation \n",
    "    content = content.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    \n",
    "    # remove isolated letters\n",
    "    content = re.sub(r'<.*?>', '', content)\n",
    "    content = re.sub(r'\\d+ *|\\b[a-z]\\b *', \"\", content) \n",
    "    content = content.strip()\n",
    "    \n",
    "    tokens = word_tokenize(content)\n",
    "\n",
    "    # lemmatization \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # clean stop words \n",
    "    stopwords = set(stpw.words(\"english\"))\n",
    " \n",
    "    # removes stopwords and duplicates\n",
    "    content = \" \".join(\n",
    "        list(dict.fromkeys([t for t in tokens if not t in stopwords]))\n",
    "    )  \n",
    "    \n",
    "    return content \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing n°2\n",
    "\n",
    "\n",
    "For this preprocessing, we used tags to remove proper nouns and verbs but removed the lemmatization. \n",
    "\n",
    "The first step in our preprocessing is to remove proper nouns and verbs to be sure that our models won't be biaised by actors'names, countries, cities or some verbs... \n",
    "\n",
    "- NNP: proper noun, singular (Harrison)\n",
    "- NNPS: proper noun, plural (Americans)\n",
    "\n",
    "\n",
    "- VB: verb base form (take)\n",
    "- VBD: verb past tense (took)\n",
    "- VBG: verb gerund/present participle (taking)\n",
    "- VBN: verb past participle (taken)\n",
    "- VBP: verb sing. present, non-3d (take)\n",
    "- VBZ: verb 3rd person sing. present (takes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_v2(content) : \n",
    "    # remove proper nouns \n",
    "    content = nltk.tag.pos_tag(content.split())\n",
    "    content = [word for word,tag in content if tag != 'NNP' and tag != 'NNPS' and tag != 'VB' and tag != 'VBD' and tag != 'VBG' and tag != 'VBN' and tag != 'VBP' and tag != 'VBZ']\n",
    "    content = \" \".join(content)\n",
    "    \n",
    "    # remove upper letters\n",
    "    content = content.lower()\n",
    "    \n",
    "    # remove punctuation \n",
    "    content = content.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    \n",
    "    # remove isolated letters\n",
    "    content = re.sub(r'<.*?>', '', content)\n",
    "    content = re.sub(r'\\d+ *|\\b[a-z]\\b *', \"\", content) \n",
    "    content = content.strip()\n",
    "    \n",
    "    tokens = word_tokenize(content)\n",
    "\n",
    "    # clean stop words \n",
    "    stopwords = set(stpw.words(\"english\")) \n",
    "    stopwords = stopwords.union(['movie'])\n",
    " \n",
    "    # removes stopwords and duplicates\n",
    "    content = \" \".join(\n",
    "        list(dict.fromkeys([t for t in tokens if not t in stopwords]))\n",
    "    )  \n",
    "    \n",
    "    return content \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizes data, apply preprocessing methods and trains classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(test_label, predicted):\n",
    "    \"\"\"\n",
    "    Computes confusion matrix and outputs the classifier's statistic\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_label : array-like of shape (n_samples,)\n",
    "\n",
    "    predicted : array-like of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(test_label, predicted).ravel() # Binary case\n",
    "    print(f\"tn : {tn}, fp : {fp}, fn : {fn}, tp : {tp}\")\n",
    "    log_stats(tn, fp, fn, tp)\n",
    "\n",
    "def log_stats(tn, fp, fn, tp):\n",
    "    \"\"\"\n",
    "    Computes accuracy, precision, recall, f1 score\n",
    "    Parameters\n",
    "    ----------\n",
    "    tn : int\n",
    "\n",
    "    fp : int\n",
    "\n",
    "    fn : int\n",
    "\n",
    "    tp : int\n",
    "    \"\"\"\n",
    "    # Accuracy\n",
    "\n",
    "    acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "    pre = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*(recall * pre) / (recall + pre)\n",
    "    \n",
    "    # Log\n",
    "\n",
    "    print(f\"Accuracy : {acc}\")\n",
    "    print(f\"Precision : {pre}\")\n",
    "    print(f\"Recall : {recall}\")\n",
    "    print(f\"F1 Score : {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing n°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Count Vectorizer : 94.08407068252563s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# use preprocessor attribute to preprocess the data, and max_features to set a boundary on the maximum number of words used to create the bag of words.\n",
    "matrix = CountVectorizer(max_features=1000, preprocessor=preprocess_v1)\n",
    "# learns the vocabulary dictionnary and returns document-term matrix as a Numpy Array.\n",
    "X_train = matrix.fit_transform(train_set.features).toarray()\n",
    "# transforms data to data-term matrix as a Numpy Array.\n",
    "X_test = matrix.transform(test_set.features).toarray()\n",
    "# gets train set labels.\n",
    "y_train = train_set.label.to_numpy()\n",
    "# gets test set labels.\n",
    "y_test = test_set.label.to_numpy()\n",
    "\n",
    "firstCVTime = time.time()\n",
    "print(f\"First Count Vectorizer : {firstCVTime - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers' trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_training(estimator, X_train, y_train):\n",
    "    cv_result = cross_validate(estimator, X_train, y_train, cv=5, return_estimator=True)\n",
    "    return cv_result['estimator'][np.argmax(cv_result['test_score'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naives Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Naives Bayes Clf : 2.769439697265625s\n",
      "tn : 10216, fp : 2284, fn : 2351, tp : 10149\n",
      "Accuracy : 0.8146\n",
      "Precision : 0.8162953430386873\n",
      "Recall : 0.81192\n",
      "F1 Score : 0.8141017928047166\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "clf = classifier_training(gnb, X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "firstNBCTime = time.time()\n",
    "print(f\"First Naives Bayes Clf : {firstNBCTime - firstCVTime}s\")\n",
    "eval(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Random Forest Clf : 59.742379665374756s\n",
      "tn : 10319, fp : 2181, fn : 2256, tp : 10244\n",
      "Accuracy : 0.82252\n",
      "Precision : 0.8244668008048289\n",
      "Recall : 0.81952\n",
      "F1 Score : 0.8219859578736207\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "clf = classifier_training(rfc, X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "firstRFCTime = time.time()\n",
    "print(f\"First Random Forest Clf : {firstRFCTime - firstNBCTime}s\")\n",
    "eval(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Support Vector Clf : 31.922729969024658s\n",
      "tn : 10462, fp : 2038, fn : 1676, tp : 10824\n",
      "Accuracy : 0.85144\n",
      "Precision : 0.8415487482506608\n",
      "Recall : 0.86592\n",
      "F1 Score : 0.8535604447598769\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "feature_map_nystroem = Nystroem(\n",
    "    random_state=1,\n",
    "    n_components=1500\n",
    ")\n",
    "transformed_X_train = feature_map_nystroem.fit_transform(X_train)\n",
    "transformed_X_test = feature_map_nystroem.transform(X_test)\n",
    "clf = classifier_training(svc, transformed_X_train, y_train)\n",
    "predicted = clf.predict(transformed_X_test)\n",
    "firstSVCTime = time.time()\n",
    "print(f\"First Support Vector Clf : {firstSVCTime - firstRFCTime}s\")\n",
    "eval(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knc = KNeighborsClassifier()\n",
    "# clf = classifier_training(knc, X_train, y_train)\n",
    "# predicted = clf.predict(X_test)\n",
    "# eval(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing n°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Count Vectorizer : 446.4436094760895s\n"
     ]
    }
   ],
   "source": [
    "# use preprocessor attribute to preprocess the data, and max_features to set a boundary on the maximum number of words used to create the bag of words.\n",
    "matrix = CountVectorizer(max_features=1000, preprocessor=preprocess_v2)\n",
    "# learns the vocabulary dictionnary and returns document-term matrix as a Numpy Array.\n",
    "X_train = matrix.fit_transform(train_set.features).toarray()\n",
    "# transforms data to data-term matrix as a Numpy Array.\n",
    "X_test = matrix.transform(test_set.features).toarray()\n",
    "# gets train set labels.\n",
    "y_train = train_set.label.to_numpy()\n",
    "# gets test set labels.\n",
    "y_test = test_set.label.to_numpy()\n",
    "\n",
    "secondCVTime = time.time()\n",
    "print(f\"Second Count Vectorizer : {secondCVTime - firstSVCTime}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers' trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naives Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Naives Bayes Clf : 2.664599657058716s\n",
      "tn : 10234, fp : 2266, fn : 2728, tp : 9772\n",
      "Accuracy : 0.80024\n",
      "Precision : 0.8117627512875893\n",
      "Recall : 0.78176\n",
      "F1 Score : 0.7964789306381939\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "clf = classifier_training(gnb, X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "secondNBCTime = time.time()\n",
    "print(f\"Second Naives Bayes Clf : {secondNBCTime - secondCVTime}s\")\n",
    "eval(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Random Forest Clf : 68.84842467308044s\n",
      "tn : 10203, fp : 2297, fn : 2374, tp : 10126\n",
      "Accuracy : 0.81316\n",
      "Precision : 0.8151010222973517\n",
      "Recall : 0.81008\n",
      "F1 Score : 0.8125827548850459\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "clf = classifier_training(rfc, X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "secondRFTime = time.time()\n",
    "print(f\"Second Random Forest Clf : {secondRFTime - secondNBCTime}s\")\n",
    "eval(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Support Vector Clf : 34.499735593795776s\n",
      "tn : 10295, fp : 2205, fn : 1747, tp : 10753\n",
      "Accuracy : 0.84192\n",
      "Precision : 0.829834851057262\n",
      "Recall : 0.86024\n",
      "F1 Score : 0.8447639248959069\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "feature_map_nystroem = Nystroem(\n",
    "    random_state=1,\n",
    "    n_components=1500\n",
    ")\n",
    "transformed_X_train = feature_map_nystroem.fit_transform(X_train)\n",
    "transformed_X_test = feature_map_nystroem.transform(X_test)\n",
    "clf = classifier_training(svc, transformed_X_train, y_train)\n",
    "predicted = clf.predict(transformed_X_test)\n",
    "secondSVCTime = time.time()\n",
    "print(f\"Second Support Vector Clf : {secondSVCTime - secondRFTime}s\")\n",
    "eval(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knc = KNeighborsClassifier()\n",
    "# clf = classifier_training(knc, X_train, y_train)\n",
    "# predicted = clf.predict(X_test)\n",
    "# eval(y_test, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
